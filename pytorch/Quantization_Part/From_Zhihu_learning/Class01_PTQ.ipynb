{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 量化示例pytorch ptq & qat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 定义简单的浮点模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def _forward_impl(self, x:Tensor) -> Tensor:\n",
    "        \"\"\"提供便捷函数\"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        x = self._forward_impl(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 定义可量化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)  将浮点模块‘M’转换为可量化模块‘QM’，（量化流程的最关键的一步）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "class QM(M):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        M (_type_): _description_\n",
    "        is_print: 为了测试需求，打印一些信息\n",
    "    \"\"\"\n",
    "    def __init__(self, is_print: bool=False):\n",
    "        super().__init__()\n",
    "        self.is_print = is_print\n",
    "        self.quant = QuantStub()  # 将张量从浮点转换为量化\n",
    "        self.dequant = DeQuantStub()  # 将张量从量化转换为浮点\n",
    "        \n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        # 手动指定张量将在量化模型中从浮点模块转换为量化模块的位置；\n",
    "        x = self.quant(x)\n",
    "        if self.is_print:\n",
    "            print(\"量化前的类型：\", x.dtype)\n",
    "        x = self._forward_impl(x)\n",
    "        if self.is_print:\n",
    "            print(\"量化中的类型：\", x.dtype)\n",
    "        # 在量化模型中手动指定张量从量化到浮点的转换位置；\n",
    "        x = self.dequant(x)\n",
    "        if self.is_print:\n",
    "            print(\"量化后的类型：\", x.dtype)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)  简单测试前向过程中的激活数据类型；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化前的类型： torch.float32\n",
      "量化中的类型： torch.float32\n",
      "量化后的类型： torch.float32\n"
     ]
    }
   ],
   "source": [
    "input_fp32 = torch.randn(4, 1, 4, 4)  # 输入的数据\n",
    "\n",
    "m = QM(is_print=True)\n",
    "x = m(input_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)  查看权重的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.conv.weight.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PTQ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当内存带宽和计算空间都很重要时，通常会使用训练后量化，而CNN就是其典型的用例。训练后量化对模型的权重和激活进行量化。他在可能的情况下将激活融合到前面的层中。他需要用具有代表性的数据集进行校准，以确定激活的最佳量化参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 静态量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建浮点模型实例\n",
    "model_fp32 = QM(is_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QM(\n",
       "  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使PTQ生效，将模型设置为eval模式\n",
    "model_fp32.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化前的类型： torch.float32\n",
      "量化中的类型： torch.float32\n",
      "量化后的类型： torch.float32\n",
      "激活和权重的数据类型分别为：torch.float32, torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 查看当前数据类型\n",
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "\n",
    "x = model_fp32(input_fp32)\n",
    "print('激活和权重的数据类型分别为：'f'{x.dtype}, {model_fp32.conv.weight.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)  定义观测器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "复制实例变量'qconfig',其中包含关于要附加哪种观测器的信息："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-使用['fbgemm']用于带AVX2的x86; 使用['qnnpack']用于ARM CPU（通常出现在移动/嵌入式设备中)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-其他量化配置，如选择对称或非对称量化和'MinMax'或'L2Norm'校准技术，可以在这里指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbjemm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化前的类型： torch.float32\n",
      "量化中的类型： torch.float32\n",
      "量化后的类型： torch.float32\n",
      "激活和权重的数据类型分别为：torch.float32, torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 查看此时的数据类型\n",
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "\n",
    "x = model_fp32(input_fp32)\n",
    "print('激活和权重的数据类型分别为：'f'{x.dtype}, {model_fp32.conv.weight.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)  融合激活层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在适用的地方，融合activation到前面的层（这需要根据模型架构手动完成）。常见的融合包括'conv+relu'和'conv+batchnorm+relu'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QM(\n",
       "  (conv): ConvReLU2d(\n",
       "    (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (relu): Identity()\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n",
    "model_fp32_fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化前的类型： torch.float32\n",
      "量化中的类型： torch.float32\n",
      "量化后的类型： torch.float32\n",
      "激活和权重的数据类型分别为：torch.float32, torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 查看此时的数据类型\n",
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "\n",
    "x = model_fp32(input_fp32)\n",
    "print('激活和权重的数据类型分别为：'f'{x.dtype}, {model_fp32.conv.weight.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)  启用观测器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在融合后的模块中启用观测器，用于在校准期间观测激活(activation)张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)  校准准备好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化前的类型： torch.float32\n",
      "量化中的类型： torch.float32\n",
      "量化后的类型： torch.float32\n",
      "激活和权重的数据类型分别为：torch.float32, torch.float32\n"
     ]
    }
   ],
   "source": [
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "\n",
    "x = model_fp32_prepared(input_fp32)\n",
    "print('激活和权重的数据类型分别为：'f'{x.dtype}, {model_fp32.conv.weight.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 模型转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化权重，计算和存储每个激活张量要适用的尺度（scale)和偏差（bias)值， 并用量化实现替换关键算子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QM(\n",
       "  (conv): QuantizedConvReLU2d(1, 1, kernel_size=(1, 1), stride=(1, 1), scale=0.011344349011778831, zero_point=0)\n",
       "  (relu): Identity()\n",
       "  (quant): Quantize(scale=tensor([0.0417]), zero_point=tensor([75]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换已经校准好的模型为量化模型\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "model_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.qint8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看权重的数据类型\n",
    "model_int8.conv.weight().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看权重的元素大小\n",
    "model_int8.conv.weight().element_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化前的类型： torch.quint8\n",
      "量化中的类型： torch.quint8\n",
      "量化后的类型： torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 运行模型，查看相关计算。相关计算将会在{data} torch.qint8中发生\n",
    "res = model_int8(input_fp32)\n",
    "res.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('pytorch_quant')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f208112fbac4f514eb3f03fffd2aee4c2c84d740ef666182a3e2003222c8aa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
