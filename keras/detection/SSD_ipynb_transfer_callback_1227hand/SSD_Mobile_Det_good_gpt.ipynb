{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 20:51:02.254124: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-27 20:51:02.285203: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-27 20:51:02.285655: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-27 20:51:02.988325: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append('./')\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import datetime\n",
    "# import keras.backend as K\n",
    "# import tensorflow as tf\n",
    "# from keras.layers import Conv2D, Dense, DepthwiseConv2D,add\n",
    "# from keras.optimizers import SGD, Adam\n",
    "# import numpy as np\n",
    "# import math\n",
    "# import keras\n",
    "# from PIL import Image\n",
    "# from random import shuffle\n",
    "# from keras import layers as KL\n",
    "from Anchors import get_anchors\n",
    "from Datasets import SSDDatasets\n",
    "# from learning_rate import WarmUpCosineDecayScheduler\n",
    "from loss import MultiboxLoss\n",
    "from Models import SSD300\n",
    "from utils import get_classes, show_config\n",
    "# from log_record import record_log, read_log\n",
    "# from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
    "#                              ModelCheckpoint, TensorBoard)\n",
    "# from callbacks import (ExponentDecayScheduler, LossHistory,\n",
    "#                        ParallelModelCheckpoint, EvalCallback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_names: ['hand'] num_classes: 2\n",
      "type: <class 'numpy.ndarray'> shape: (1242, 4)\n",
      "Train on 1440 samples, val on 178 samples, with batch size 16.\n",
      "Configurations:\n",
      "----------------------------------------------------------------------\n",
      "|                     keys |                                   values|\n",
      "----------------------------------------------------------------------\n",
      "|             classes_path |             ./model_data/voc_classes.txt|\n",
      "|               model_path |                                         |\n",
      "|              input_shape |                               [120, 160]|\n",
      "|                    Epoch |                                      500|\n",
      "|               batch_size |                                       16|\n",
      "|                       lr |                                   0.0001|\n",
      "|           optimizer_type |                                     Adam|\n",
      "|                 momentum |                                    0.937|\n",
      "|                num_train |                                     1440|\n",
      "|                  num_val |                                      178|\n",
      "----------------------------------------------------------------------\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 20:10:45.427066: W tensorflow/c/c_api.cc:304] Operation '{name:'DepthwiseConv2D_conf_DD5_1/bias/Assign' id:1484 op device:{requested: '', assigned: ''} def:{{{node DepthwiseConv2D_conf_DD5_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](DepthwiseConv2D_conf_DD5_1/bias, DepthwiseConv2D_conf_DD5_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4/90 [>.............................] - ETA: 5s - batch: 1.5000 - size: 16.0000 - loss: 7.1671     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 20:10:46.695305: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2024-06-27 20:10:46.695364: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2024-06-27 20:10:46.756634: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2024-06-27 20:10:46.761818: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2024-06-27 20:10:46.764167: I tensorflow/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: /home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_1227hand/output/20240228/unetlogs/plugins/profile/2024_06_27_20_10_46/SSSLXSRVLS001.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 0s - batch: 44.5000 - size: 16.0000 - loss: 6.7485"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangyouan/anaconda3/envs/stc/lib/python3.10/site-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2024-06-27 20:10:51.917921: W tensorflow/c/c_api.cc:304] Operation '{name:'loss/AddN' id:1775 op device:{requested: '', assigned: ''} def:{{{node loss/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul, loss/Conv2D_layer13/kernel/Regularizer/mul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 12s 96ms/step - batch: 44.5000 - size: 16.0000 - loss: 6.7485 - val_loss: 6.1902 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "90/90 [==============================] - 6s 67ms/step - batch: 44.5000 - size: 16.0000 - loss: 5.8294 - val_loss: 5.6375 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 5.5655 - val_loss: 5.4875 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 5.4713 - val_loss: 5.4503 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 5.4450 - val_loss: 5.4170 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 5.4313 - val_loss: 5.3899 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "90/90 [==============================] - 6s 66ms/step - batch: 44.5000 - size: 16.0000 - loss: 5.4176 - val_loss: 5.3685 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 5.4024 - val_loss: 5.3773 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 5.3785 - val_loss: 5.2786 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "90/90 [==============================] - 6s 66ms/step - batch: 44.5000 - size: 16.0000 - loss: 5.0619 - val_loss: 4.8084 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 4.6761 - val_loss: 4.4145 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 4.3561 - val_loss: 4.2026 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 4.0665 - val_loss: 3.8867 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 3.7968 - val_loss: 3.5658 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 3.5717 - val_loss: 3.4958 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 3.3817 - val_loss: 3.3913 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 3.3223 - val_loss: 3.2469 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "90/90 [==============================] - 4s 45ms/step - batch: 44.5000 - size: 16.0000 - loss: 3.1975 - val_loss: 3.1335 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "90/90 [==============================] - 5s 56ms/step - batch: 44.5000 - size: 16.0000 - loss: 3.0580 - val_loss: 2.9740 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.9881 - val_loss: 2.9953 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.8706 - val_loss: 2.8627 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.7417 - val_loss: 2.7117 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.7167 - val_loss: 2.6899 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.6204 - val_loss: 2.6565 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.5598 - val_loss: 2.5605 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "90/90 [==============================] - 6s 66ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.4735 - val_loss: 2.5047 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.4039 - val_loss: 2.4498 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.3713 - val_loss: 2.3939 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "90/90 [==============================] - 5s 56ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.2944 - val_loss: 2.3971 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "90/90 [==============================] - 4s 47ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.2502 - val_loss: 2.4143 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "90/90 [==============================] - 6s 68ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.2302 - val_loss: 2.3492 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "90/90 [==============================] - 6s 66ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.1667 - val_loss: 2.3240 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.1629 - val_loss: 2.2677 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.0970 - val_loss: 2.2524 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "90/90 [==============================] - 6s 66ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.0635 - val_loss: 2.2318 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.0848 - val_loss: 2.2180 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "90/90 [==============================] - 5s 50ms/step - batch: 44.5000 - size: 16.0000 - loss: 2.0578 - val_loss: 2.1715 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "90/90 [==============================] - 4s 49ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.9821 - val_loss: 2.2176 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.9660 - val_loss: 2.1946 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.8950 - val_loss: 2.1498 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.9103 - val_loss: 2.1849 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.9121 - val_loss: 2.1229 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.8402 - val_loss: 2.1915 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.8324 - val_loss: 2.1134 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "90/90 [==============================] - 4s 47ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.8284 - val_loss: 2.1370 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "90/90 [==============================] - 5s 51ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.7985 - val_loss: 2.1045 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.7722 - val_loss: 2.0855 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.8023 - val_loss: 2.1037 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.7304 - val_loss: 2.2574 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.7159 - val_loss: 2.1266 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.7170 - val_loss: 2.3090 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.7007 - val_loss: 2.0811 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "90/90 [==============================] - 4s 45ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.6789 - val_loss: 2.0888 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "90/90 [==============================] - 5s 58ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.6351 - val_loss: 2.0650 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "90/90 [==============================] - 6s 66ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.6361 - val_loss: 2.0607 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.6180 - val_loss: 2.0261 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.6013 - val_loss: 2.1824 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "90/90 [==============================] - 4s 44ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.5823 - val_loss: 2.0433 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "90/90 [==============================] - 5s 53ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.5567 - val_loss: 2.0452 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "90/90 [==============================] - 5s 50ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.5613 - val_loss: 2.0606 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "90/90 [==============================] - 4s 47ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.5450 - val_loss: 2.0886 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.5121 - val_loss: 1.9840 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.5285 - val_loss: 1.9744 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "90/90 [==============================] - 6s 62ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.5000 - val_loss: 2.0188 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.4992 - val_loss: 2.0178 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.4868 - val_loss: 2.1362 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "90/90 [==============================] - 6s 66ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.5216 - val_loss: 1.9404 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.4889 - val_loss: 2.1526 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.4852 - val_loss: 2.0438 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.4524 - val_loss: 2.0302 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.4308 - val_loss: 2.0171 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.4597 - val_loss: 2.0497 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.3849 - val_loss: 1.9727 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.4115 - val_loss: 1.9904 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.4115 - val_loss: 1.9989 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.3840 - val_loss: 2.0450 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.4166 - val_loss: 2.1125 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.3457 - val_loss: 2.0148 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.3761 - val_loss: 1.9878 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.2996 - val_loss: 1.9769 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.3282 - val_loss: 2.0227 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.3261 - val_loss: 2.0540 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.2873 - val_loss: 2.0307 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.2824 - val_loss: 2.0701 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.3290 - val_loss: 2.1997 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.4132 - val_loss: 1.9816 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "90/90 [==============================] - ETA: 0s - batch: 44.5000 - size: 16.0000 - loss: 1.2663\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "90/90 [==============================] - 6s 66ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.2663 - val_loss: 2.0462 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1714 - val_loss: 1.9626 - lr: 3.0000e-04\n",
      "Epoch 89/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1446 - val_loss: 1.9620 - lr: 3.0000e-04\n",
      "Epoch 90/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1573 - val_loss: 2.0316 - lr: 3.0000e-04\n",
      "Epoch 91/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1433 - val_loss: 1.9653 - lr: 3.0000e-04\n",
      "Epoch 92/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1413 - val_loss: 1.9895 - lr: 3.0000e-04\n",
      "Epoch 93/500\n",
      "90/90 [==============================] - 6s 62ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1249 - val_loss: 1.9825 - lr: 3.0000e-04\n",
      "Epoch 94/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1325 - val_loss: 1.9699 - lr: 3.0000e-04\n",
      "Epoch 95/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1337 - val_loss: 1.9751 - lr: 3.0000e-04\n",
      "Epoch 96/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1320 - val_loss: 1.9889 - lr: 3.0000e-04\n",
      "Epoch 97/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1282 - val_loss: 1.9707 - lr: 3.0000e-04\n",
      "Epoch 98/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1165 - val_loss: 1.9787 - lr: 3.0000e-04\n",
      "Epoch 99/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1216 - val_loss: 1.9903 - lr: 3.0000e-04\n",
      "Epoch 100/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1084 - val_loss: 1.9988 - lr: 3.0000e-04\n",
      "Epoch 101/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1019 - val_loss: 1.9759 - lr: 3.0000e-04\n",
      "Epoch 102/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1074 - val_loss: 1.9972 - lr: 3.0000e-04\n",
      "Epoch 103/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0922 - val_loss: 1.9824 - lr: 3.0000e-04\n",
      "Epoch 104/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0927 - val_loss: 1.9972 - lr: 3.0000e-04\n",
      "Epoch 105/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0921 - val_loss: 2.0043 - lr: 3.0000e-04\n",
      "Epoch 106/500\n",
      "90/90 [==============================] - 7s 72ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0883 - val_loss: 1.9941 - lr: 3.0000e-04\n",
      "Epoch 107/500\n",
      "90/90 [==============================] - ETA: 0s - batch: 44.5000 - size: 16.0000 - loss: 1.1062\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.1062 - val_loss: 2.0118 - lr: 3.0000e-04\n",
      "Epoch 108/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0569 - val_loss: 1.9869 - lr: 9.0000e-05\n",
      "Epoch 109/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0553 - val_loss: 2.0042 - lr: 9.0000e-05\n",
      "Epoch 110/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0623 - val_loss: 1.9904 - lr: 9.0000e-05\n",
      "Epoch 111/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0442 - val_loss: 2.0056 - lr: 9.0000e-05\n",
      "Epoch 112/500\n",
      "90/90 [==============================] - 6s 65ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0478 - val_loss: 2.0062 - lr: 9.0000e-05\n",
      "Epoch 113/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0475 - val_loss: 2.0086 - lr: 9.0000e-05\n",
      "Epoch 114/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0434 - val_loss: 2.0136 - lr: 9.0000e-05\n",
      "Epoch 115/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0439 - val_loss: 2.0173 - lr: 9.0000e-05\n",
      "Epoch 116/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0415 - val_loss: 2.0237 - lr: 9.0000e-05\n",
      "Epoch 117/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0437 - val_loss: 2.0191 - lr: 9.0000e-05\n",
      "Epoch 118/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0438 - val_loss: 2.0168 - lr: 9.0000e-05\n",
      "Epoch 119/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0483 - val_loss: 2.0100 - lr: 9.0000e-05\n",
      "Epoch 120/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0429 - val_loss: 1.9942 - lr: 9.0000e-05\n",
      "Epoch 121/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0418 - val_loss: 2.0251 - lr: 9.0000e-05\n",
      "Epoch 122/500\n",
      "90/90 [==============================] - 6s 62ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0385 - val_loss: 2.0202 - lr: 9.0000e-05\n",
      "Epoch 123/500\n",
      "90/90 [==============================] - 4s 41ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0344 - val_loss: 2.0163 - lr: 9.0000e-05\n",
      "Epoch 124/500\n",
      "90/90 [==============================] - 5s 59ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0348 - val_loss: 2.0271 - lr: 9.0000e-05\n",
      "Epoch 125/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0377 - val_loss: 2.0141 - lr: 9.0000e-05\n",
      "Epoch 126/500\n",
      "90/90 [==============================] - 4s 42ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0341 - val_loss: 2.0192 - lr: 9.0000e-05\n",
      "Epoch 127/500\n",
      "90/90 [==============================] - ETA: 0s - batch: 44.5000 - size: 16.0000 - loss: 1.0292\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
      "90/90 [==============================] - 5s 58ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0292 - val_loss: 2.0227 - lr: 9.0000e-05\n",
      "Epoch 128/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0166 - val_loss: 2.0168 - lr: 2.7000e-05\n",
      "Epoch 129/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0224 - val_loss: 2.0177 - lr: 2.7000e-05\n",
      "Epoch 130/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0184 - val_loss: 2.0148 - lr: 2.7000e-05\n",
      "Epoch 131/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0201 - val_loss: 2.0232 - lr: 2.7000e-05\n",
      "Epoch 132/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0263 - val_loss: 2.0163 - lr: 2.7000e-05\n",
      "Epoch 133/500\n",
      "90/90 [==============================] - 4s 44ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0206 - val_loss: 2.0170 - lr: 2.7000e-05\n",
      "Epoch 134/500\n",
      "90/90 [==============================] - 5s 55ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0178 - val_loss: 2.0198 - lr: 2.7000e-05\n",
      "Epoch 135/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0148 - val_loss: 2.0233 - lr: 2.7000e-05\n",
      "Epoch 136/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0161 - val_loss: 2.0236 - lr: 2.7000e-05\n",
      "Epoch 137/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0126 - val_loss: 2.0198 - lr: 2.7000e-05\n",
      "Epoch 138/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0144 - val_loss: 2.0199 - lr: 2.7000e-05\n",
      "Epoch 139/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0219 - val_loss: 2.0219 - lr: 2.7000e-05\n",
      "Epoch 140/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0240 - val_loss: 2.0220 - lr: 2.7000e-05\n",
      "Epoch 141/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0176 - val_loss: 2.0173 - lr: 2.7000e-05\n",
      "Epoch 142/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0126 - val_loss: 2.0235 - lr: 2.7000e-05\n",
      "Epoch 143/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0200 - val_loss: 2.0275 - lr: 2.7000e-05\n",
      "Epoch 144/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0195 - val_loss: 2.0201 - lr: 2.7000e-05\n",
      "Epoch 145/500\n",
      "90/90 [==============================] - 6s 64ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0107 - val_loss: 2.0236 - lr: 2.7000e-05\n",
      "Epoch 146/500\n",
      "90/90 [==============================] - 6s 63ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0191 - val_loss: 2.0230 - lr: 2.7000e-05\n",
      "Epoch 147/500\n",
      "89/90 [============================>.] - ETA: 0s - batch: 44.0000 - size: 16.0000 - loss: 1.0142\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 8.100000013655517e-06.\n",
      "90/90 [==============================] - 6s 62ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0149 - val_loss: 2.0336 - lr: 2.7000e-05\n",
      "Epoch 148/500\n",
      "90/90 [==============================] - 4s 41ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0057 - val_loss: 2.0268 - lr: 8.1000e-06\n",
      "Epoch 149/500\n",
      "90/90 [==============================] - 5s 61ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0130 - val_loss: 2.0265 - lr: 8.1000e-06\n",
      "Epoch 150/500\n",
      "90/90 [==============================] - 4s 43ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0091 - val_loss: 2.0240 - lr: 8.1000e-06\n",
      "Epoch 151/500\n",
      "90/90 [==============================] - 4s 40ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0115 - val_loss: 2.0255 - lr: 8.1000e-06\n",
      "Epoch 152/500\n",
      "90/90 [==============================] - 4s 41ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0155 - val_loss: 2.0258 - lr: 8.1000e-06\n",
      "Epoch 153/500\n",
      "90/90 [==============================] - 5s 60ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0165 - val_loss: 2.0254 - lr: 8.1000e-06\n",
      "Epoch 154/500\n",
      "90/90 [==============================] - 4s 41ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0098 - val_loss: 2.0257 - lr: 8.1000e-06\n",
      "Epoch 155/500\n",
      "90/90 [==============================] - 4s 48ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0054 - val_loss: 2.0272 - lr: 8.1000e-06\n",
      "Epoch 156/500\n",
      "90/90 [==============================] - 4s 44ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0148 - val_loss: 2.0246 - lr: 8.1000e-06\n",
      "Epoch 157/500\n",
      "90/90 [==============================] - 5s 53ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0102 - val_loss: 2.0223 - lr: 8.1000e-06\n",
      "Epoch 158/500\n",
      "90/90 [==============================] - 4s 41ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0090 - val_loss: 2.0238 - lr: 8.1000e-06\n",
      "Epoch 159/500\n",
      "90/90 [==============================] - 5s 56ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0130 - val_loss: 2.0251 - lr: 8.1000e-06\n",
      "Epoch 160/500\n",
      "90/90 [==============================] - 4s 41ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0117 - val_loss: 2.0284 - lr: 8.1000e-06\n",
      "Epoch 161/500\n",
      "90/90 [==============================] - 4s 43ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0046 - val_loss: 2.0259 - lr: 8.1000e-06\n",
      "Epoch 162/500\n",
      "90/90 [==============================] - 4s 47ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0151 - val_loss: 2.0273 - lr: 8.1000e-06\n",
      "Epoch 163/500\n",
      "90/90 [==============================] - 4s 41ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0086 - val_loss: 2.0248 - lr: 8.1000e-06\n",
      "Epoch 164/500\n",
      "90/90 [==============================] - 7s 72ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0107 - val_loss: 2.0272 - lr: 8.1000e-06\n",
      "Epoch 165/500\n",
      "90/90 [==============================] - 4s 41ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0127 - val_loss: 2.0284 - lr: 8.1000e-06\n",
      "Epoch 166/500\n",
      "90/90 [==============================] - 7s 74ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0092 - val_loss: 2.0243 - lr: 8.1000e-06\n",
      "Epoch 167/500\n",
      "89/90 [============================>.] - ETA: 0s - batch: 44.0000 - size: 16.0000 - loss: 1.0045\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-06.\n",
      "90/90 [==============================] - 4s 44ms/step - batch: 44.5000 - size: 16.0000 - loss: 1.0033 - val_loss: 2.0261 - lr: 8.1000e-06\n",
      "Epoch 167: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangyouan/anaconda3/envs/stc/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_1227hand/output/20240228/hand_detection_20240626_gpt.pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_1227hand/output/20240228/hand_detection_20240626_gpt.pb/assets\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 设置训练参数\n",
    "    Epoch = 500  # 训练100 epochs\n",
    "    lr = 1e-4  # Adam优化器，所以较小的学习率\n",
    "    optimizer_type = \"Adam\"\n",
    "    momentum = 0.937\n",
    "    batch_size = 16\n",
    "    imgcolor = 'grey'  # imgcolor选“rgb” or “grey”, 则处理图像变单通道或者三通道\n",
    "    tmp_dir = str(datetime.datetime.strftime(datetime.datetime.now(), '%Y%m%d'))\n",
    "    save_dir = \"/home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_1227hand/output/20240228\"\n",
    "        \n",
    "    # 设置SSD参数\n",
    "    cls_name_path = \"./model_data/voc_classes.txt\"  # 导入目标检测类别；\n",
    "    input_shape = [120, 160]  # 输入的尺寸大小\n",
    "    anchor_size = [32, 59, 86, 113, 141, 168]  # 用于设定先验框的大小，根据公式计算而来；如果要检测小物体，修改浅层先验框的大小，越小的话，识别的物体越小；    \n",
    "    train_annotation_path = r'/home/zhangyouan/桌面/zya/dataset/681/hand/2007_train.txt'  # 训练图片路径和标签\n",
    "    val_annotation_path = r'/home/zhangyouan/桌面/zya/dataset/681/hand/2007_test.txt'  # 验证图片路径和标签\n",
    "        \n",
    "    # 1. 获取classes和anchor\n",
    "    class_names, num_cls = get_classes(cls_name_path)\n",
    "    num_cls += 1  # 增加一个背景类别\n",
    "    print(\"class_names:\", class_names, \"num_classes:\", num_cls)\n",
    "    \n",
    "    # 2. 获取anchors, 输出的是归一化之后的anchors\n",
    "    anchor = get_anchors(input_shape, anchor_size)\n",
    "    print(\"type:\",type(anchor), \"shape:\", np.shape(anchor))\n",
    "\n",
    "    # 3. 模型编译\n",
    "    K.clear_session()\n",
    "    model_path = \"\"\n",
    "    # model_path = \"./output/20230804_3/good_detection_test_callback.h5\"\n",
    "    model = SSD300((input_shape[0], input_shape[1], 1), num_cls)\n",
    "    # model.save(\"template.h5\")\n",
    "    # model.summary()\n",
    "    if model_path != \"\":\n",
    "        model.load_weights(model_path, by_name = True, skip_mismatch=True)\n",
    "       \n",
    "    # 4. 优化器\n",
    "    # optimizer = Adam(lr = lr, beta_1=momentum)\n",
    "    # optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    from tensorflow.keras.optimizers import legacy\n",
    "    optimizer = legacy.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0)\n",
    "    \n",
    "    # 5. 导入数据集\n",
    "    with open(train_annotation_path, encoding='utf-8') as f:\n",
    "        train_lines = f.readlines()\n",
    "    with open(val_annotation_path, encoding='utf-8') as f:\n",
    "        val_lines = f.readlines()\n",
    "    num_train = len(train_lines)\n",
    "    num_val = len(val_lines)\n",
    "    epoch_step = num_train // batch_size\n",
    "    epoch_step_val = num_val // batch_size\n",
    "    \n",
    "    # 数据增强设置\n",
    "    train_dataloader = SSDDatasets(train_lines, input_shape, anchor, batch_size, num_cls, train=False, imgcolor=imgcolor)\n",
    "    val_dataloader = SSDDatasets(val_lines, input_shape, anchor, batch_size, num_cls, train=False, imgcolor=imgcolor)\n",
    "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "    \n",
    "    # 6. 编译模型\n",
    "    # losses = {'mbox_loc_final':MultiboxLoss(num_cls, neg_pos_ratio=3.0).compute_loc_loss,\n",
    "            #   'cls_conf_final':MultiboxLoss(num_cls, neg_pos_ratio=3.0).compute_conf_loss}\n",
    "    # losses2 = [MultiboxLoss(num_cls, neg_pos_ratio=3.0).compute_loc_loss,MultiboxLoss(num_cls, neg_pos_ratio=3.0).compute_conf_loss]\n",
    "    model.compile(optimizer=optimizer, loss = MultiboxLoss(num_cls, neg_pos_ratio=3.0).compute_loss)\n",
    "    \n",
    "    # # 8. 精度评价: pending --> 还没构建；\n",
    "    # eval_flag = True\n",
    "    # eval_period = 10\n",
    "    # eval_callback = EvalCallback(model, input_shape, anchor, class_names, num_cls, val_lines, log_dir, eval_flag=eval_flag, period = eval_period)\n",
    "    show_config(\n",
    "        classes_path=cls_name_path, model_path=model_path, input_shape=input_shape, \\\n",
    "        Epoch=Epoch, batch_size=batch_size, \\\n",
    "        lr=lr, optimizer_type=optimizer_type, momentum=momentum, \\\n",
    "        num_train=num_train, num_val=num_val\n",
    "    )\n",
    "    checkpoint = ModelCheckpoint('ssd_weights.h5', monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=20, verbose=1)\n",
    "    csv_logger = CSVLogger('training.log')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n",
    "\n",
    "    # 8. 开始训练；\n",
    "    history = model.fit(\n",
    "            train_dataloader,\n",
    "            validation_data=val_dataloader,\n",
    "            steps_per_epoch=epoch_step,\n",
    "            validation_steps=epoch_step_val,\n",
    "            epochs=Epoch,\n",
    "            callbacks=[checkpoint,reduce_lr, csv_logger, early_stopping, \n",
    "                       keras.callbacks.TensorBoard(log_dir=os.path.join(save_dir, 'unetlogs'), update_freq=1000)],\n",
    "        )\n",
    "    \n",
    "    record_log(history, filename = os.path.join(save_dir, \"unetlogs/log.txt\"))\n",
    "    model.save(os.path.join(save_dir, \"hand_detection_20240626_gpt.h5\"))\n",
    "    model.save(os.path.join(save_dir, \"hand_detection_20240626_gpt.pb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no accuracy, only loss.\n"
     ]
    }
   ],
   "source": [
    "from log_record import record_log, read_log\n",
    "from utils import visual_train\n",
    "\n",
    "history = read_log(\"/home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback/output/20230819/unetlogs/log.txt\")\n",
    "visual_train(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_0919_cola/output/20231205/unetlogs/log.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_1227hand/SSD_Mobile_Det_good_gpt.ipynb Cell 4\u001b[0m line \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22494d58227d/home/zhangyouan/%E6%A1%8C%E9%9D%A2/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_1227hand/SSD_Mobile_Det_good_gpt.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlog_record\u001b[39;00m \u001b[39mimport\u001b[39;00m record_log, read_log\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22494d58227d/home/zhangyouan/%E6%A1%8C%E9%9D%A2/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_1227hand/SSD_Mobile_Det_good_gpt.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m visual_train\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22494d58227d/home/zhangyouan/%E6%A1%8C%E9%9D%A2/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_1227hand/SSD_Mobile_Det_good_gpt.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m history \u001b[39m=\u001b[39m read_log(\u001b[39m\"\u001b[39;49m\u001b[39m/home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_0919_cola/output/20231205/unetlogs/log.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22494d58227d/home/zhangyouan/%E6%A1%8C%E9%9D%A2/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_1227hand/SSD_Mobile_Det_good_gpt.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m visual_train(history)\n",
      "File \u001b[0;32m~/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_1227hand/log_record.py:11\u001b[0m, in \u001b[0;36mread_log\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_log\u001b[39m(filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlog.txt\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m         history \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m     13\u001b[0m         \u001b[39mreturn\u001b[39;00m history\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_0919_cola/output/20231205/unetlogs/log.txt'"
     ]
    }
   ],
   "source": [
    "from log_record import record_log, read_log\n",
    "from utils import visual_train\n",
    "\n",
    "history = read_log(\"/home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_0919_cola/output/20231205/unetlogs/log.txt\")\n",
    "visual_train(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_names: ['good'] num_classes: 2\n",
      "type: <class 'numpy.ndarray'> shape: (1242, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 346 samples, val on 346 samples, with batch size 16.\n",
      "Epoch 1/1000\n",
      "21/21 [==============================] - 6s 79ms/step - loss: 1.3499 - val_loss: 1.2268 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 1.3878 - val_loss: 1.1425 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.3449 - val_loss: 1.1460 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1994 - val_loss: 1.1620 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 1.2621 - val_loss: 1.1304 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2656 - val_loss: 1.1756 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.3280 - val_loss: 1.1635 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2918 - val_loss: 1.1914 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.5193 - val_loss: 1.1515 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.3216 - val_loss: 1.1620 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.3399 - val_loss: 1.2662 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2642 - val_loss: 1.1659 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2681 - val_loss: 1.2039 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2598 - val_loss: 1.1415 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 1.3531 - val_loss: 1.1108 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 1.2946 - val_loss: 1.1025 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.4481 - val_loss: 1.1108 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.3621 - val_loss: 1.2034 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.3377 - val_loss: 1.3177 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.3857 - val_loss: 1.2657 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.3770 - val_loss: 1.1282 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 1.2663 - val_loss: 1.1024 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1962 - val_loss: 1.1425 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2978 - val_loss: 1.1121 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2570 - val_loss: 1.1708 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3072\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.3072 - val_loss: 1.2277 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.3225 - val_loss: 1.1464 - lr: 2.0000e-04\n",
      "Epoch 28/1000\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 1.2363 - val_loss: 1.0605 - lr: 2.0000e-04\n",
      "Epoch 29/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1644 - val_loss: 1.1225 - lr: 2.0000e-04\n",
      "Epoch 30/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2802 - val_loss: 1.0742 - lr: 2.0000e-04\n",
      "Epoch 31/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2446 - val_loss: 1.0644 - lr: 2.0000e-04\n",
      "Epoch 32/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1928 - val_loss: 1.0841 - lr: 2.0000e-04\n",
      "Epoch 33/1000\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 1.1438 - val_loss: 1.0326 - lr: 2.0000e-04\n",
      "Epoch 34/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2658 - val_loss: 1.0586 - lr: 2.0000e-04\n",
      "Epoch 35/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1829 - val_loss: 1.0704 - lr: 2.0000e-04\n",
      "Epoch 36/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2811 - val_loss: 1.0541 - lr: 2.0000e-04\n",
      "Epoch 37/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1755 - val_loss: 1.0622 - lr: 2.0000e-04\n",
      "Epoch 38/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1478 - val_loss: 1.0407 - lr: 2.0000e-04\n",
      "Epoch 39/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2873 - val_loss: 1.0525 - lr: 2.0000e-04\n",
      "Epoch 40/1000\n",
      "21/21 [==============================] - 1s 59ms/step - loss: 1.2037 - val_loss: 1.0283 - lr: 2.0000e-04\n",
      "Epoch 41/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2172 - val_loss: 1.0435 - lr: 2.0000e-04\n",
      "Epoch 42/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1541 - val_loss: 1.0524 - lr: 2.0000e-04\n",
      "Epoch 43/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2578 - val_loss: 1.0927 - lr: 2.0000e-04\n",
      "Epoch 44/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1820 - val_loss: 1.1066 - lr: 2.0000e-04\n",
      "Epoch 45/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1557 - val_loss: 1.0533 - lr: 2.0000e-04\n",
      "Epoch 46/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2803 - val_loss: 1.0458 - lr: 2.0000e-04\n",
      "Epoch 47/1000\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 1.1834 - val_loss: 1.0228 - lr: 2.0000e-04\n",
      "Epoch 48/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0771 - val_loss: 1.0501 - lr: 2.0000e-04\n",
      "Epoch 49/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1853 - val_loss: 1.0676 - lr: 2.0000e-04\n",
      "Epoch 50/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1773 - val_loss: 1.1047 - lr: 2.0000e-04\n",
      "Epoch 51/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1749 - val_loss: 1.0599 - lr: 2.0000e-04\n",
      "Epoch 52/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2013 - val_loss: 1.0544 - lr: 2.0000e-04\n",
      "Epoch 53/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.2300 - val_loss: 1.0402 - lr: 2.0000e-04\n",
      "Epoch 54/1000\n",
      "21/21 [==============================] - 1s 59ms/step - loss: 1.1589 - val_loss: 1.0199 - lr: 2.0000e-04\n",
      "Epoch 55/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1918 - val_loss: 1.0720 - lr: 2.0000e-04\n",
      "Epoch 56/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1133 - val_loss: 1.0397 - lr: 2.0000e-04\n",
      "Epoch 57/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1989 - val_loss: 1.0731 - lr: 2.0000e-04\n",
      "Epoch 58/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1666 - val_loss: 1.0499 - lr: 2.0000e-04\n",
      "Epoch 59/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.1091 - val_loss: 1.0585 - lr: 2.0000e-04\n",
      "Epoch 60/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1143 - val_loss: 1.0627 - lr: 2.0000e-04\n",
      "Epoch 61/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1376 - val_loss: 1.0555 - lr: 2.0000e-04\n",
      "Epoch 62/1000\n",
      "21/21 [==============================] - 1s 59ms/step - loss: 1.1556 - val_loss: 1.0080 - lr: 2.0000e-04\n",
      "Epoch 63/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1404 - val_loss: 1.0160 - lr: 2.0000e-04\n",
      "Epoch 64/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.2148 - val_loss: 1.0388 - lr: 2.0000e-04\n",
      "Epoch 65/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2098 - val_loss: 1.0316 - lr: 2.0000e-04\n",
      "Epoch 66/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1059 - val_loss: 1.0607 - lr: 2.0000e-04\n",
      "Epoch 67/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2167 - val_loss: 1.0625 - lr: 2.0000e-04\n",
      "Epoch 68/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1870 - val_loss: 1.0411 - lr: 2.0000e-04\n",
      "Epoch 69/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1361 - val_loss: 1.0424 - lr: 2.0000e-04\n",
      "Epoch 70/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2405 - val_loss: 1.0408 - lr: 2.0000e-04\n",
      "Epoch 71/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.1096 - val_loss: 1.0149 - lr: 2.0000e-04\n",
      "Epoch 72/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1042\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.1042 - val_loss: 1.0210 - lr: 2.0000e-04\n",
      "Epoch 73/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.0555 - val_loss: 1.0284 - lr: 4.0000e-05\n",
      "Epoch 74/1000\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 1.2224 - val_loss: 1.0042 - lr: 4.0000e-05\n",
      "Epoch 75/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1023 - val_loss: 1.0141 - lr: 4.0000e-05\n",
      "Epoch 76/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1155 - val_loss: 1.0179 - lr: 4.0000e-05\n",
      "Epoch 77/1000\n",
      "21/21 [==============================] - 1s 59ms/step - loss: 1.1323 - val_loss: 1.0017 - lr: 4.0000e-05\n",
      "Epoch 78/1000\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 1.1393 - val_loss: 0.9967 - lr: 4.0000e-05\n",
      "Epoch 79/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2488 - val_loss: 1.0067 - lr: 4.0000e-05\n",
      "Epoch 80/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1968 - val_loss: 1.0118 - lr: 4.0000e-05\n",
      "Epoch 81/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1957 - val_loss: 1.0143 - lr: 4.0000e-05\n",
      "Epoch 82/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1879 - val_loss: 1.0151 - lr: 4.0000e-05\n",
      "Epoch 83/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.1287 - val_loss: 1.0410 - lr: 4.0000e-05\n",
      "Epoch 84/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1014 - val_loss: 1.0190 - lr: 4.0000e-05\n",
      "Epoch 85/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1215 - val_loss: 1.0271 - lr: 4.0000e-05\n",
      "Epoch 86/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1675 - val_loss: 1.0218 - lr: 4.0000e-05\n",
      "Epoch 87/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1093 - val_loss: 1.0218 - lr: 4.0000e-05\n",
      "Epoch 88/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1728\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1728 - val_loss: 1.0233 - lr: 4.0000e-05\n",
      "Epoch 89/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1693 - val_loss: 1.0098 - lr: 8.0000e-06\n",
      "Epoch 90/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1550 - val_loss: 1.0087 - lr: 8.0000e-06\n",
      "Epoch 91/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.2470 - val_loss: 1.0063 - lr: 8.0000e-06\n",
      "Epoch 92/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1595 - val_loss: 1.0101 - lr: 8.0000e-06\n",
      "Epoch 93/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2022 - val_loss: 1.0080 - lr: 8.0000e-06\n",
      "Epoch 94/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2394 - val_loss: 0.9972 - lr: 8.0000e-06\n",
      "Epoch 95/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1489 - val_loss: 1.0158 - lr: 8.0000e-06\n",
      "Epoch 96/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.1649 - val_loss: 1.0201 - lr: 8.0000e-06\n",
      "Epoch 97/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1092 - val_loss: 1.0161 - lr: 8.0000e-06\n",
      "Epoch 98/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1935\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1935 - val_loss: 1.0089 - lr: 8.0000e-06\n",
      "Epoch 99/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0865 - val_loss: 1.0122 - lr: 1.6000e-06\n",
      "Epoch 100/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1475 - val_loss: 1.0152 - lr: 1.6000e-06\n",
      "Epoch 101/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0724 - val_loss: 1.0158 - lr: 1.6000e-06\n",
      "Epoch 102/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1564 - val_loss: 1.0133 - lr: 1.6000e-06\n",
      "Epoch 103/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1460 - val_loss: 1.0090 - lr: 1.6000e-06\n",
      "Epoch 104/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.1191 - val_loss: 1.0248 - lr: 1.6000e-06\n",
      "Epoch 105/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2630 - val_loss: 1.0069 - lr: 1.6000e-06\n",
      "Epoch 106/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2630 - val_loss: 1.0168 - lr: 1.6000e-06\n",
      "Epoch 107/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2445 - val_loss: 1.0114 - lr: 1.6000e-06\n",
      "Epoch 108/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0920\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.0920 - val_loss: 1.0119 - lr: 1.6000e-06\n",
      "Epoch 109/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0908 - val_loss: 1.0135 - lr: 3.2000e-07\n",
      "Epoch 110/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1740 - val_loss: 1.0187 - lr: 3.2000e-07\n",
      "Epoch 111/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1379 - val_loss: 1.0055 - lr: 3.2000e-07\n",
      "Epoch 112/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0956 - val_loss: 1.0219 - lr: 3.2000e-07\n",
      "Epoch 113/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2511 - val_loss: 1.0121 - lr: 3.2000e-07\n",
      "Epoch 114/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1529 - val_loss: 1.0273 - lr: 3.2000e-07\n",
      "Epoch 115/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1756 - val_loss: 1.0165 - lr: 3.2000e-07\n",
      "Epoch 116/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1313 - val_loss: 1.0134 - lr: 3.2000e-07\n",
      "Epoch 117/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.1099 - val_loss: 1.0107 - lr: 3.2000e-07\n",
      "Epoch 118/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1740\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1740 - val_loss: 1.0105 - lr: 3.2000e-07\n",
      "Epoch 119/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2079 - val_loss: 1.0126 - lr: 6.4000e-08\n",
      "Epoch 120/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.0600 - val_loss: 1.0127 - lr: 6.4000e-08\n",
      "Epoch 121/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2012 - val_loss: 1.0142 - lr: 6.4000e-08\n",
      "Epoch 122/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.0895 - val_loss: 1.0136 - lr: 6.4000e-08\n",
      "Epoch 123/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.0987 - val_loss: 1.0139 - lr: 6.4000e-08\n",
      "Epoch 124/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.0788 - val_loss: 1.0024 - lr: 6.4000e-08\n",
      "Epoch 125/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1007 - val_loss: 1.0148 - lr: 6.4000e-08\n",
      "Epoch 126/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1625 - val_loss: 1.0110 - lr: 6.4000e-08\n",
      "Epoch 127/1000\n",
      "21/21 [==============================] - 1s 60ms/step - loss: 1.1752 - val_loss: 0.9888 - lr: 6.4000e-08\n",
      "Epoch 128/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1327 - val_loss: 1.0121 - lr: 6.4000e-08\n",
      "Epoch 129/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1862 - val_loss: 1.0234 - lr: 6.4000e-08\n",
      "Epoch 130/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1829 - val_loss: 1.0130 - lr: 6.4000e-08\n",
      "Epoch 131/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1636 - val_loss: 1.0161 - lr: 6.4000e-08\n",
      "Epoch 132/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2131 - val_loss: 1.0112 - lr: 6.4000e-08\n",
      "Epoch 133/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.1169 - val_loss: 1.0203 - lr: 6.4000e-08\n",
      "Epoch 134/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.2204 - val_loss: 1.0181 - lr: 6.4000e-08\n",
      "Epoch 135/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1827 - val_loss: 1.0212 - lr: 6.4000e-08\n",
      "Epoch 136/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1309 - val_loss: 1.0143 - lr: 6.4000e-08\n",
      "Epoch 137/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1263\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1263 - val_loss: 1.0207 - lr: 6.4000e-08\n",
      "Epoch 138/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2250 - val_loss: 1.0173 - lr: 1.2800e-08\n",
      "Epoch 139/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1505 - val_loss: 1.0105 - lr: 1.2800e-08\n",
      "Epoch 140/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1593 - val_loss: 1.0130 - lr: 1.2800e-08\n",
      "Epoch 141/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1676 - val_loss: 1.0174 - lr: 1.2800e-08\n",
      "Epoch 142/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2258 - val_loss: 1.0122 - lr: 1.2800e-08\n",
      "Epoch 143/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.1934 - val_loss: 1.0221 - lr: 1.2800e-08\n",
      "Epoch 144/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1478 - val_loss: 1.0086 - lr: 1.2800e-08\n",
      "Epoch 145/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1599 - val_loss: 1.0236 - lr: 1.2800e-08\n",
      "Epoch 146/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1821 - val_loss: 1.0167 - lr: 1.2800e-08\n",
      "Epoch 147/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1483\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1483 - val_loss: 1.0189 - lr: 1.2800e-08\n",
      "Epoch 148/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.0737 - val_loss: 1.0049 - lr: 2.5600e-09\n",
      "Epoch 149/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2138 - val_loss: 1.0023 - lr: 2.5600e-09\n",
      "Epoch 150/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.0486 - val_loss: 1.0169 - lr: 2.5600e-09\n",
      "Epoch 151/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2011 - val_loss: 1.0012 - lr: 2.5600e-09\n",
      "Epoch 152/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1028 - val_loss: 1.0116 - lr: 2.5600e-09\n",
      "Epoch 153/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2237 - val_loss: 1.0144 - lr: 2.5600e-09\n",
      "Epoch 154/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1602 - val_loss: 1.0180 - lr: 2.5600e-09\n",
      "Epoch 155/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1421 - val_loss: 1.0117 - lr: 2.5600e-09\n",
      "Epoch 156/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1778 - val_loss: 1.0191 - lr: 2.5600e-09\n",
      "Epoch 157/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2108\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2108 - val_loss: 1.0067 - lr: 2.5600e-09\n",
      "Epoch 158/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.1622 - val_loss: 1.0235 - lr: 5.1200e-10\n",
      "Epoch 159/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0890 - val_loss: 1.0123 - lr: 5.1200e-10\n",
      "Epoch 160/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1834 - val_loss: 1.0121 - lr: 5.1200e-10\n",
      "Epoch 161/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1403 - val_loss: 1.0106 - lr: 5.1200e-10\n",
      "Epoch 162/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1698 - val_loss: 1.0156 - lr: 5.1200e-10\n",
      "Epoch 163/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1047 - val_loss: 1.0108 - lr: 5.1200e-10\n",
      "Epoch 164/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0756 - val_loss: 1.0076 - lr: 5.1200e-10\n",
      "Epoch 165/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1093 - val_loss: 1.0157 - lr: 5.1200e-10\n",
      "Epoch 166/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.0964 - val_loss: 1.0292 - lr: 5.1200e-10\n",
      "Epoch 167/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1409\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1409 - val_loss: 1.0188 - lr: 5.1200e-10\n",
      "Epoch 168/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2365 - val_loss: 1.0161 - lr: 1.0240e-10\n",
      "Epoch 169/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1439 - val_loss: 1.0118 - lr: 1.0240e-10\n",
      "Epoch 170/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1456 - val_loss: 1.0270 - lr: 1.0240e-10\n",
      "Epoch 171/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.0736 - val_loss: 1.0121 - lr: 1.0240e-10\n",
      "Epoch 172/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1009 - val_loss: 1.0105 - lr: 1.0240e-10\n",
      "Epoch 173/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2212 - val_loss: 1.0212 - lr: 1.0240e-10\n",
      "Epoch 174/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.0986 - val_loss: 1.0165 - lr: 1.0240e-10\n",
      "Epoch 175/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1356 - val_loss: 1.0258 - lr: 1.0240e-10\n",
      "Epoch 176/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1101 - val_loss: 1.0005 - lr: 1.0240e-10\n",
      "Epoch 177/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1496\n",
      "Epoch 177: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1496 - val_loss: 1.0107 - lr: 1.0240e-10\n",
      "Epoch 178/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1831 - val_loss: 1.0152 - lr: 2.0480e-11\n",
      "Epoch 179/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1919 - val_loss: 1.0036 - lr: 2.0480e-11\n",
      "Epoch 180/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1437 - val_loss: 1.0152 - lr: 2.0480e-11\n",
      "Epoch 181/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1253 - val_loss: 1.0042 - lr: 2.0480e-11\n",
      "Epoch 182/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1250 - val_loss: 1.0061 - lr: 2.0480e-11\n",
      "Epoch 183/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1894 - val_loss: 1.0054 - lr: 2.0480e-11\n",
      "Epoch 184/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1254 - val_loss: 1.0265 - lr: 2.0480e-11\n",
      "Epoch 185/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1028 - val_loss: 1.0063 - lr: 2.0480e-11\n",
      "Epoch 186/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1626 - val_loss: 1.0118 - lr: 2.0480e-11\n",
      "Epoch 187/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1585\n",
      "Epoch 187: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1585 - val_loss: 1.0268 - lr: 2.0480e-11\n",
      "Epoch 188/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1269 - val_loss: 1.0251 - lr: 4.0960e-12\n",
      "Epoch 189/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1796 - val_loss: 1.0078 - lr: 4.0960e-12\n",
      "Epoch 190/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1114 - val_loss: 1.0092 - lr: 4.0960e-12\n",
      "Epoch 191/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1084 - val_loss: 1.0152 - lr: 4.0960e-12\n",
      "Epoch 192/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1414 - val_loss: 1.0163 - lr: 4.0960e-12\n",
      "Epoch 193/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1856 - val_loss: 1.0243 - lr: 4.0960e-12\n",
      "Epoch 194/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1898 - val_loss: 1.0179 - lr: 4.0960e-12\n",
      "Epoch 195/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2516 - val_loss: 1.0115 - lr: 4.0960e-12\n",
      "Epoch 196/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2033 - val_loss: 1.0125 - lr: 4.0960e-12\n",
      "Epoch 197/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1619\n",
      "Epoch 197: ReduceLROnPlateau reducing learning rate to 8.192000897078167e-13.\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1619 - val_loss: 1.0098 - lr: 4.0960e-12\n",
      "Epoch 198/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1757 - val_loss: 1.0076 - lr: 8.1920e-13\n",
      "Epoch 199/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1217 - val_loss: 0.9952 - lr: 8.1920e-13\n",
      "Epoch 200/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1330 - val_loss: 1.0164 - lr: 8.1920e-13\n",
      "Epoch 201/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1142 - val_loss: 1.0082 - lr: 8.1920e-13\n",
      "Epoch 202/1000\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.0939 - val_loss: 1.0163 - lr: 8.1920e-13\n",
      "Epoch 203/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.2146 - val_loss: 1.0151 - lr: 8.1920e-13\n",
      "Epoch 204/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1479 - val_loss: 1.0098 - lr: 8.1920e-13\n",
      "Epoch 205/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1717 - val_loss: 1.0227 - lr: 8.1920e-13\n",
      "Epoch 206/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1797 - val_loss: 1.0067 - lr: 8.1920e-13\n",
      "Epoch 207/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1503\n",
      "Epoch 207: ReduceLROnPlateau reducing learning rate to 1.6384001360475466e-13.\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1503 - val_loss: 1.0185 - lr: 8.1920e-13\n",
      "Epoch 208/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1176 - val_loss: 1.0320 - lr: 1.6384e-13\n",
      "Epoch 209/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0589 - val_loss: 1.0296 - lr: 1.6384e-13\n",
      "Epoch 210/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1677 - val_loss: 1.0176 - lr: 1.6384e-13\n",
      "Epoch 211/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1319 - val_loss: 1.0059 - lr: 1.6384e-13\n",
      "Epoch 212/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1554 - val_loss: 1.0197 - lr: 1.6384e-13\n",
      "Epoch 213/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1139 - val_loss: 1.0116 - lr: 1.6384e-13\n",
      "Epoch 214/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1803 - val_loss: 1.0148 - lr: 1.6384e-13\n",
      "Epoch 215/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1405 - val_loss: 1.0050 - lr: 1.6384e-13\n",
      "Epoch 216/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2323 - val_loss: 1.0121 - lr: 1.6384e-13\n",
      "Epoch 217/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1350\n",
      "Epoch 217: ReduceLROnPlateau reducing learning rate to 3.2768002178849846e-14.\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1350 - val_loss: 1.0165 - lr: 1.6384e-13\n",
      "Epoch 218/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1262 - val_loss: 1.0219 - lr: 3.2768e-14\n",
      "Epoch 219/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1615 - val_loss: 1.0260 - lr: 3.2768e-14\n",
      "Epoch 220/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0967 - val_loss: 1.0123 - lr: 3.2768e-14\n",
      "Epoch 221/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1292 - val_loss: 1.0207 - lr: 3.2768e-14\n",
      "Epoch 222/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.0689 - val_loss: 1.0113 - lr: 3.2768e-14\n",
      "Epoch 223/1000\n",
      "21/21 [==============================] - 1s 54ms/step - loss: 1.1187 - val_loss: 1.0226 - lr: 3.2768e-14\n",
      "Epoch 224/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2043 - val_loss: 1.0093 - lr: 3.2768e-14\n",
      "Epoch 225/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1002 - val_loss: 0.9972 - lr: 3.2768e-14\n",
      "Epoch 226/1000\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1506 - val_loss: 1.0131 - lr: 3.2768e-14\n",
      "Epoch 227/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1456\n",
      "Epoch 227: ReduceLROnPlateau reducing learning rate to 6.553600300244697e-15.\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1456 - val_loss: 1.0068 - lr: 3.2768e-14\n",
      "Epoch 227: early stopping\n",
      "Model training completed and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # 设置训练参数\n",
    "        Epoch = 1000  # 训练1000 epochs\n",
    "        lr = 1e-4  # Adam优化器的学习率\n",
    "        momentum = 0.937\n",
    "        batch_size = 16\n",
    "        imgcolor = 'grey'  # 图像处理的颜色空间\n",
    "\n",
    "        # 设置SSD参数\n",
    "        # cls_name_path = '/home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/VOC_dataset/voc_classes.txt'  # 类别文件路径\n",
    "        cls_name_path = r\"/home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/VOC_dataset/voc_classes.txt\"\n",
    "        input_shape = [120, 160]  # 输入尺寸\n",
    "        anchor_size = [32, 59, 86, 113, 141, 168]  # 先验框大小    \n",
    "        # train_annotation_path = '/home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/VOC_dataset/2007_train.txt'\n",
    "        # val_annotation_path = '/home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/VOC_dataset/2007_val.txt'\n",
    "        # train_annotation_path = r'/home/zhangyouan/桌面/zya/dataset/681/hand/2007_train.txt'\n",
    "        # val_annotation_path = r'/home/zhangyouan/桌面/zya/dataset/681/hand/2007_test.txt'\n",
    "        train_annotation_path = r\"/home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/VOC_dataset/2007_train.txt\"\n",
    "        val_annotation_path = r\"/home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/VOC_dataset/2007_val.txt\"\n",
    "        # 1. 获取classes和anchors\n",
    "        class_names, num_cls = get_classes(cls_name_path)\n",
    "        num_cls += 1  # 增加背景类别\n",
    "        print(\"class_names:\", class_names, \"num_classes:\", num_cls)\n",
    "        \n",
    "        # 2. 获取anchors\n",
    "        anchor = get_anchors(input_shape, anchor_size)\n",
    "        print(\"type:\", type(anchor), \"shape:\", np.shape(anchor))\n",
    "\n",
    "        # 3. 清理session并创建模型\n",
    "        K.clear_session()\n",
    "        model = SSD300((input_shape[0], input_shape[1], 1), num_cls)\n",
    "        \n",
    "        # 4. 设置优化器\n",
    "        optimizer = Adam(lr=lr, beta_1=momentum)\n",
    "        \n",
    "        # 5. 导入数据集\n",
    "        with open(train_annotation_path, encoding='utf-8') as f:\n",
    "            train_lines = f.readlines()\n",
    "        with open(val_annotation_path, encoding='utf-8') as f:\n",
    "            val_lines = f.readlines()\n",
    "        num_train = len(train_lines)\n",
    "        num_val = len(val_lines)\n",
    "        epoch_step = num_train // batch_size\n",
    "        epoch_step_val = num_val // batch_size\n",
    "\n",
    "        # 数据增强设置\n",
    "        train_dataloader = SSDDatasets(train_lines, input_shape, anchor, batch_size, num_cls, train=True, imgcolor=imgcolor)\n",
    "        val_dataloader = SSDDatasets(val_lines, input_shape, anchor, batch_size, num_cls, train=False, imgcolor=imgcolor)\n",
    "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "        \n",
    "        # 6. 编译模型\n",
    "        model.compile(optimizer=optimizer, loss=MultiboxLoss(num_cls, neg_pos_ratio=3.0).compute_loss)\n",
    "        # 7. 预训练权重\n",
    "        model.load_weights(r\"/home/zhangyouan/桌面/zya/NN_net/network/SSD/IMX_681_ssd_mobilenet_git/keras/detection/SSD_ipynb_transfer_callback_1227hand/final_det_good_train_1_0860_val_1_0166.h5\")\n",
    "        # 7. 设置回调函数\n",
    "        checkpoint = ModelCheckpoint('ssd_weights.h5', monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, verbose=1)\n",
    "        csv_logger = CSVLogger('training.log')\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n",
    "\n",
    "        # 8. 训练模型\n",
    "        model.fit(\n",
    "            train_dataloader,\n",
    "            validation_data=val_dataloader,\n",
    "            steps_per_epoch=epoch_step,\n",
    "            validation_steps=epoch_step_val,\n",
    "            epochs=Epoch,\n",
    "            callbacks=[checkpoint, reduce_lr, csv_logger, early_stopping]\n",
    "        )\n",
    "\n",
    "        # 9. 保存最终模型\n",
    "        model.save(\"final_detection_model_det_good_tmp.h5\")\n",
    "        print(\"Model training completed and saved successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('stc': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6a050d9f10360465fbc02ae273ccd06cb1948ad5cd96cc14a3b3a9694a266bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
